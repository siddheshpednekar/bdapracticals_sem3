{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddheshpednekar/bdapracticals_sem3/blob/main/P6_MLib_Feature_Extraction%2CTransformation_%26_Selecting_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF(Term frequency-inverse document frequency)\n",
        "#TF: Both HashingTF and CountVectorizer can be used to generate the term frequency vectors.\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Feature Extractors",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "0f561c7f-b637-454d-a732-6cb7623a619e"
        },
        "id": "NHF_YNVEvF1p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sentenceData = spark.createDataFrame([\n",
        "    (0.0, \"Hi I heard about Spark\"),\n",
        "    (0.0, \"I wish Java could use case classes\"),\n",
        "    (1.0, \"Logistic regression models are neat\")\n",
        "], [\"label\", \"sentence\"])\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "\n",
        "rescaledData.select(\"label\", \"features\").show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7628e987-2bea-4ccd-a09b-faf66eed9f2d"
        },
        "id": "3KCFm16IvF1s",
        "outputId": "e3c7f4f6-78ff-4ae9-b145-12cdeeaee580"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  0.0|(20,[6,8,13,16],[...|\n|  0.0|(20,[0,2,7,13,15,...|\n|  1.0|(20,[3,4,6,11,19]...|\n+-----+--------------------+\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  0.0|(20,[6,8,13,16],[...|\n|  0.0|(20,[0,2,7,13,15,...|\n|  1.0|(20,[3,4,6,11,19]...|\n+-----+--------------------+\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Word2Vec\n",
        "\n",
        "# Input data: Each row is a bag of words from a sentence or document.\n",
        "documentDF = spark.createDataFrame([\n",
        "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
        "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
        "    (\"Logistic regression models are neat\".split(\" \"), )\n",
        "], [\"text\"])\n",
        "\n",
        "# Learn a mapping from words to Vectors.\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
        "model = word2Vec.fit(documentDF)\n",
        "\n",
        "result = model.transform(documentDF)\n",
        "for row in result.collect():\n",
        "    text, vector = row\n",
        "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Word2Vec",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "fa243ed9-d776-4d0d-9735-9e5deb9a8f7d"
        },
        "id": "ZLwim-stvF1t",
        "outputId": "888fda67-fce0-49ee-cbf6-216df95806fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "Text: [Hi, I, heard, about, Spark] => \nVector: [0.012264367192983627,-0.06442034244537354,-0.007622340321540833]\n\nText: [I, wish, Java, could, use, case, classes] => \nVector: [0.05160687722465289,0.025969027541577816,0.02736483487699713]\n\nText: [Logistic, regression, models, are, neat] => \nVector: [-0.06564115285873413,0.02060299552977085,-0.08455150425434113]\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "Text: [Hi, I, heard, about, Spark] => \nVector: [0.012264367192983627,-0.06442034244537354,-0.007622340321540833]\n\nText: [I, wish, Java, could, use, case, classes] => \nVector: [0.05160687722465289,0.025969027541577816,0.02736483487699713]\n\nText: [Logistic, regression, models, are, neat] => \nVector: [-0.06564115285873413,0.02060299552977085,-0.08455150425434113]\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# Input data: Each row is a bag of words with a ID.\n",
        "df = spark.createDataFrame([\n",
        "    (0, \"a b c\".split(\" \")),\n",
        "    (1, \"a b b c a\".split(\" \"))\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "# fit a CountVectorizerModel from the corpus.\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
        "\n",
        "model = cv.fit(df)\n",
        "\n",
        "result = model.transform(df)\n",
        "result.show(truncate=False)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "CountVectorizer",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "64ab6444-d1ed-451b-9bde-8ac6ad696d7d"
        },
        "id": "F7VfxkCsvF1t",
        "outputId": "a06b24b5-415a-4e1f-b557-f51a06a881be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "+---+---------------+-------------------------+\n|id |words          |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "+---+---------------+-------------------------+\n|id |words          |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Feature Transformers",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "78240d57-d4de-45a5-a134-65995e0f1464"
        },
        "id": "dV2A8kA_vF1u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "sentenceDataFrame = spark.createDataFrame([\n",
        "    (0, \"Hi I heard about Spark\"),\n",
        "    (1, \"I wish Java could use case classes\"),\n",
        "    (2, \"Logistic,regression,models,are,neat\")\n",
        "], [\"id\", \"sentence\"])\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
        "\n",
        "countTokens = udf(lambda words: len(words), IntegerType())\n",
        "\n",
        "tokenized = tokenizer.transform(sentenceDataFrame)\n",
        "tokenized.select(\"sentence\", \"words\")\\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
        "\n",
        "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
        "regexTokenized.select(\"sentence\", \"words\") \\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Tokenizer",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "26feb813-7810-4228-84bd-7757a5d71991"
        },
        "id": "F39DkoJKvF1u",
        "outputId": "3ede9392-1adf-4f2c-d918-3f577c77602b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "+-----------------------------------+------------------------------------------+------+\n|sentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\n|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n+-----------------------------------+------------------------------------------+------+\n\n+-----------------------------------+------------------------------------------+------+\n|sentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\n|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n+-----------------------------------+------------------------------------------+------+\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "+-----------------------------------+------------------------------------------+------+\n|sentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\n|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n+-----------------------------------+------------------------------------------+------+\n\n+-----------------------------------+------------------------------------------+------+\n|sentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\n|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n+-----------------------------------+------------------------------------------+------+\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
        "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
        "], [\"id\", \"raw\"])\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
        "remover.transform(sentenceData).show(truncate=False)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "StopWordsRemover",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "d64563a0-ed3d-466d-a4c1-0d50133c9a95"
        },
        "id": "UItDtKfXvF1v",
        "outputId": "ca92f7e9-17fd-477a-f979-de9aeb90bf35"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "+---+----------------------------+--------------------+\n|id |raw                         |filtered            |\n+---+----------------------------+--------------------+\n|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n+---+----------------------------+--------------------+\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "+---+----------------------------+--------------------+\n|id |raw                         |filtered            |\n+---+----------------------------+--------------------+\n|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n+---+----------------------------+--------------------+\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import NGram\n",
        "\n",
        "wordDataFrame = spark.createDataFrame([\n",
        "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
        "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
        "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
        "\n",
        "ngramDataFrame = ngram.transform(wordDataFrame)\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "n -gram",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "add251be-17c0-4966-af8e-c37fb9d69a1e"
        },
        "id": "GsgSyNzevF1w",
        "outputId": "d48a9889-f721-4c3e-9065-916d0f500378"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "+------------------------------------------------------------------+\n|ngrams                                                            |\n+------------------------------------------------------------------+\n|[Hi I, I heard, heard about, about Spark]                         |\n|[I wish, wish Java, Java could, could use, use case, case classes]|\n|[Logistic regression, regression models, models are, are neat]    |\n+------------------------------------------------------------------+\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "+------------------------------------------------------------------+\n|ngrams                                                            |\n+------------------------------------------------------------------+\n|[Hi I, I heard, heard about, about Spark]                         |\n|[I wish, wish Java, Java could, could use, use case, case classes]|\n|[Logistic regression, regression models, models are, are neat]    |\n+------------------------------------------------------------------+\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Binarizer\n",
        "\n",
        "continuousDataFrame = spark.createDataFrame([\n",
        "    (0, 0.1),\n",
        "    (1, 0.8),\n",
        "    (2, 0.2)\n",
        "], [\"id\", \"feature\"])\n",
        "\n",
        "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
        "\n",
        "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
        "\n",
        "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
        "binarizedDataFrame.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Binarizer",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "389ecfe8-0f4e-4461-afdd-db1f7088306c"
        },
        "id": "KzEvrCZJvF1w",
        "outputId": "e33a5efc-8183-43d7-efda-d2d5af8474be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "Binarizer output with Threshold = 0.500000\n+---+-------+-----------------+\n| id|feature|binarized_feature|\n+---+-------+-----------------+\n|  0|    0.1|              0.0|\n|  1|    0.8|              1.0|\n|  2|    0.2|              0.0|\n+---+-------+-----------------+\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "Binarizer output with Threshold = 0.500000\n+---+-------+-----------------+\n| id|feature|binarized_feature|\n+---+-------+-----------------+\n|  0|    0.1|              0.0|\n|  1|    0.8|              1.0|\n|  2|    0.2|              0.0|\n+---+-------+-----------------+\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
        "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
        "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
        "df = spark.createDataFrame(data, [\"features\"])\n",
        "\n",
        "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
        "model = pca.fit(df)\n",
        "\n",
        "result = model.transform(df).select(\"pcaFeatures\")\n",
        "result.show(truncate=False)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "PCA",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "6e91ecd8-d840-4d84-9a0a-7f50d8e23d36"
        },
        "id": "R6wFTflLvF1x",
        "outputId": "d7b26481-d0d5-44a2-8b84-26c361bd3c65"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "+------------------------------------------------------------+\n|pcaFeatures                                                 |\n+------------------------------------------------------------+\n|[1.6485728230883814,-4.0132827005162985,-1.0091435193998504]|\n|[-4.645104331781533,-1.1167972663619048,-1.0091435193998501]|\n|[-6.428880535676488,-5.337951427775359,-1.009143519399851]  |\n+------------------------------------------------------------+\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "+------------------------------------------------------------+\n|pcaFeatures                                                 |\n+------------------------------------------------------------+\n|[1.6485728230883814,-4.0132827005162985,-1.0091435193998504]|\n|[-4.645104331781533,-1.1167972663619048,-1.0091435193998501]|\n|[-6.428880535676488,-5.337951427775359,-1.009143519399851]  |\n+------------------------------------------------------------+\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import PolynomialExpansion\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (Vectors.dense([2.0, 1.0]),),\n",
        "    (Vectors.dense([0.0, 0.0]),),\n",
        "    (Vectors.dense([3.0, -1.0]),)\n",
        "], [\"features\"])\n",
        "\n",
        "polyExpansion = PolynomialExpansion(degree=3, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
        "polyDF = polyExpansion.transform(df)\n",
        "\n",
        "polyDF.show(truncate=False)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "PolynomialExpansion",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "06708cc2-69dc-4248-95e1-53638a03fb45"
        },
        "id": "_PxgsQ-fvF1x",
        "outputId": "0cca8b3a-5026-48df-eb9f-e6d15e313aa6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "+----------+------------------------------------------+\n|features  |polyFeatures                              |\n+----------+------------------------------------------+\n|[2.0,1.0] |[2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]     |\n|[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]     |\n|[3.0,-1.0]|[3.0,9.0,27.0,-1.0,-3.0,-9.0,1.0,3.0,-1.0]|\n+----------+------------------------------------------+\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "+----------+------------------------------------------+\n|features  |polyFeatures                              |\n+----------+------------------------------------------+\n|[2.0,1.0] |[2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]     |\n|[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]     |\n|[3.0,-1.0]|[3.0,9.0,27.0,-1.0,-3.0,-9.0,1.0,3.0,-1.0]|\n+----------+------------------------------------------+\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (0.0, 1.0),\n",
        "    (1.0, 0.0),\n",
        "    (2.0, 1.0),\n",
        "    (0.0, 2.0),\n",
        "    (0.0, 1.0),\n",
        "    (2.0, 0.0)\n",
        "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
        "\n",
        "encoder = OneHotEncoder(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
        "                        outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
        "model = encoder.fit(df)\n",
        "encoded = model.transform(df)\n",
        "encoded.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "OneHotEncoder",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "7efb74ee-0ef3-4ef8-a709-328614ca8c49"
        },
        "id": "ne65HVpFvF1y",
        "outputId": "ed6af658-6933-4667-c1db-fcb5cf34d527"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "+--------------+--------------+-------------+-------------+\n|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n+--------------+--------------+-------------+-------------+\n|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n+--------------+--------------+-------------+-------------+\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "+--------------+--------------+-------------+-------------+\n|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n+--------------+--------------+-------------+-------------+\n|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n+--------------+--------------+-------------+-------------+\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Normalizer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "dataFrame = spark.createDataFrame([\n",
        "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
        "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
        "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
        "], [\"id\", \"features\"])\n",
        "\n",
        "# Normalize each Vector using $L^1$ norm.\n",
        "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
        "l1NormData = normalizer.transform(dataFrame)\n",
        "print(\"Normalized using L^1 norm\")\n",
        "l1NormData.show()\n",
        "\n",
        "# Normalize each Vector using $L^\\infty$ norm.\n",
        "lInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\n",
        "print(\"Normalized using L^inf norm\")\n",
        "lInfNormData.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Normalizer",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "2338cfdc-8660-4ee0-8b3e-edf7b22dc033"
        },
        "id": "Xmi4v3FcvF1z",
        "outputId": "1864afea-e292-4f62-86db-23bdefcea799"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "Normalized using L^1 norm\n+---+--------------+------------------+\n| id|      features|      normFeatures|\n+---+--------------+------------------+\n|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n+---+--------------+------------------+\n\nNormalized using L^inf norm\n+---+--------------+--------------+\n| id|      features|  normFeatures|\n+---+--------------+--------------+\n|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n+---+--------------+--------------+\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "Normalized using L^1 norm\n+---+--------------+------------------+\n| id|      features|      normFeatures|\n+---+--------------+------------------+\n|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n+---+--------------+------------------+\n\nNormalized using L^inf norm\n+---+--------------+--------------+\n| id|      features|  normFeatures|\n+---+--------------+--------------+\n|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n+---+--------------+--------------+\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Feature Selectors ",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "3d993217-626e-40ee-9859-5755f98c3b64"
        },
        "id": "vVJqVJ4XvF1z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import ChiSqSelector\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
        "    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
        "    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
        "\n",
        "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n",
        "                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
        "\n",
        "result = selector.fit(df).transform(df)\n",
        "\n",
        "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
        "result.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "ChiSqSelector",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "e70983f8-3f43-465b-ab49-24c5a906fdf4"
        },
        "id": "uVVXYYlkvF1z",
        "outputId": "63fe2493-1ce5-465e-dc71-d9999626d3fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "ChiSqSelector output with top 1 features selected\n+---+------------------+-------+----------------+\n| id|          features|clicked|selectedFeatures|\n+---+------------------+-------+----------------+\n|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n+---+------------------+-------+----------------+\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "ChiSqSelector output with top 1 features selected\n+---+------------------+-------+----------------+\n| id|          features|clicked|selectedFeatures|\n+---+------------------+-------+----------------+\n|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n+---+------------------+-------+----------------+\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import UnivariateFeatureSelector\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (1, Vectors.dense([1.7, 4.4, 7.6, 5.8, 9.6, 2.3]), 3.0,),\n",
        "    (2, Vectors.dense([8.8, 7.3, 5.7, 7.3, 2.2, 4.1]), 2.0,),\n",
        "    (3, Vectors.dense([1.2, 9.5, 2.5, 3.1, 8.7, 2.5]), 3.0,),\n",
        "    (4, Vectors.dense([3.7, 9.2, 6.1, 4.1, 7.5, 3.8]), 2.0,),\n",
        "    (5, Vectors.dense([8.9, 5.2, 7.8, 8.3, 5.2, 3.0]), 4.0,),\n",
        "    (6, Vectors.dense([7.9, 8.5, 9.2, 4.0, 9.4, 2.1]), 4.0,)], [\"id\", \"features\", \"label\"])\n",
        "\n",
        "selector = UnivariateFeatureSelector(featuresCol=\"features\", outputCol=\"selectedFeatures\",\n",
        "                                     labelCol=\"label\", selectionMode=\"numTopFeatures\")\n",
        "selector.setFeatureType(\"continuous\").setLabelType(\"categorical\").setSelectionThreshold(1)\n",
        "\n",
        "result = selector.fit(df).transform(df)\n",
        "\n",
        "print(\"UnivariateFeatureSelector output with top %d features selected using f_classif\"\n",
        "      % selector.getSelectionThreshold())\n",
        "result.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "UnivariateFeatureSelector",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "42372cfe-65e6-4642-9b8a-9c6d6757a5b7"
        },
        "id": "MPYtHLmovF10",
        "outputId": "5f5df6ba-ea38-40a1-9d91-6881c3599c13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "datasetInfos": [],
              "data": "UnivariateFeatureSelector output with top 1 features selected using f_classif\n+---+--------------------+-----+----------------+\n| id|            features|label|selectedFeatures|\n+---+--------------------+-----+----------------+\n|  1|[1.7,4.4,7.6,5.8,...|  3.0|           [2.3]|\n|  2|[8.8,7.3,5.7,7.3,...|  2.0|           [4.1]|\n|  3|[1.2,9.5,2.5,3.1,...|  3.0|           [2.5]|\n|  4|[3.7,9.2,6.1,4.1,...|  2.0|           [3.8]|\n|  5|[8.9,5.2,7.8,8.3,...|  4.0|           [3.0]|\n|  6|[7.9,8.5,9.2,4.0,...|  4.0|           [2.1]|\n+---+--------------------+-----+----------------+\n\n",
              "removedWidgets": [],
              "addedWidgets": {},
              "metadata": {},
              "type": "ansi",
              "arguments": {}
            }
          },
          "data": {
            "text/plain": [
              "UnivariateFeatureSelector output with top 1 features selected using f_classif\n+---+--------------------+-----+----------------+\n| id|            features|label|selectedFeatures|\n+---+--------------------+-----+----------------+\n|  1|[1.7,4.4,7.6,5.8,...|  3.0|           [2.3]|\n|  2|[8.8,7.3,5.7,7.3,...|  2.0|           [4.1]|\n|  3|[1.2,9.5,2.5,3.1,...|  3.0|           [2.5]|\n|  4|[3.7,9.2,6.1,4.1,...|  2.0|           [3.8]|\n|  5|[8.9,5.2,7.8,8.3,...|  4.0|           [3.0]|\n|  6|[7.9,8.5,9.2,4.0,...|  4.0|           [2.1]|\n+---+--------------------+-----+----------------+\n\n"
            ]
          }
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "PySpark MLib:Feature Extraction,Transformation & Selecting Features",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 2682825334014532
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}