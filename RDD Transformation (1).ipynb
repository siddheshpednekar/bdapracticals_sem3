{"cells":[{"cell_type":"code","source":["%python\n#https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-transformations/\n#https://sparkbyexamples.com/pyspark/pyspark-rdd-transformations/\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Rdd_trans').getOrCreate()\nrdd = spark.sparkContext.textFile('dbfs:/FileStore/shared_uploads/AKSHATAKHEDEKAR01032001@rjcollege.edu.in/text-3.txt')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c44ba616-136a-4d0c-b7e0-2a90278c68c9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for element in rdd.collect():\n    print(element)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5b6e20e-3ac3-4d81-95fe-3c9710bd69e8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Spark revolves around the concept of a resilient distributed dataset (RDD), \nwhich is a fault-tolerant collection of elements that can be operated on in parallel. \nThere are two ways to create RDDs: parallelizing an existing collection in your driver program, \nor referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, \nor any data source offering a Hadoop InputFormat.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Spark revolves around the concept of a resilient distributed dataset (RDD), \nwhich is a fault-tolerant collection of elements that can be operated on in parallel. \nThere are two ways to create RDDs: parallelizing an existing collection in your driver program, \nor referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, \nor any data source offering a Hadoop InputFormat.\n"]}}],"execution_count":0},{"cell_type":"code","source":["#Flatmap\nrdd2 = rdd.flatMap(lambda x: x.split(\" \"))\nfor element in rdd2.collect():\n    print(element)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d64c30bc-70f1-4f13-bd19-e9cfa44de5d5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Spark\nrevolves\naround\nthe\nconcept\nof\na\nresilient\ndistributed\ndataset\n(RDD),\n\nwhich\nis\na\nfault-tolerant\ncollection\nof\nelements\nthat\ncan\nbe\noperated\non\nin\nparallel.\n\nThere\nare\ntwo\nways\nto\ncreate\nRDDs:\nparallelizing\nan\nexisting\ncollection\nin\nyour\ndriver\nprogram,\n\nor\nreferencing\na\ndataset\nin\nan\nexternal\nstorage\nsystem,\nsuch\nas\na\nshared\nfilesystem,\nHDFS,\nHBase,\n\nor\nany\ndata\nsource\noffering\na\nHadoop\nInputFormat.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Spark\nrevolves\naround\nthe\nconcept\nof\na\nresilient\ndistributed\ndataset\n(RDD),\n\nwhich\nis\na\nfault-tolerant\ncollection\nof\nelements\nthat\ncan\nbe\noperated\non\nin\nparallel.\n\nThere\nare\ntwo\nways\nto\ncreate\nRDDs:\nparallelizing\nan\nexisting\ncollection\nin\nyour\ndriver\nprogram,\n\nor\nreferencing\na\ndataset\nin\nan\nexternal\nstorage\nsystem,\nsuch\nas\na\nshared\nfilesystem,\nHDFS,\nHBase,\n\nor\nany\ndata\nsource\noffering\na\nHadoop\nInputFormat.\n"]}}],"execution_count":0},{"cell_type":"code","source":["#Map\nrdd3 = rdd2.map(lambda x: (x,1))\nfor element in rdd3.collect():\n    print(element)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe70265d-200e-485b-b2ba-aec507e1fda9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"('Spark', 1)\n('revolves', 1)\n('around', 1)\n('the', 1)\n('concept', 1)\n('of', 1)\n('a', 1)\n('resilient', 1)\n('distributed', 1)\n('dataset', 1)\n('(RDD),', 1)\n('', 1)\n('which', 1)\n('is', 1)\n('a', 1)\n('fault-tolerant', 1)\n('collection', 1)\n('of', 1)\n('elements', 1)\n('that', 1)\n('can', 1)\n('be', 1)\n('operated', 1)\n('on', 1)\n('in', 1)\n('parallel.', 1)\n('', 1)\n('There', 1)\n('are', 1)\n('two', 1)\n('ways', 1)\n('to', 1)\n('create', 1)\n('RDDs:', 1)\n('parallelizing', 1)\n('an', 1)\n('existing', 1)\n('collection', 1)\n('in', 1)\n('your', 1)\n('driver', 1)\n('program,', 1)\n('', 1)\n('or', 1)\n('referencing', 1)\n('a', 1)\n('dataset', 1)\n('in', 1)\n('an', 1)\n('external', 1)\n('storage', 1)\n('system,', 1)\n('such', 1)\n('as', 1)\n('a', 1)\n('shared', 1)\n('filesystem,', 1)\n('HDFS,', 1)\n('HBase,', 1)\n('', 1)\n('or', 1)\n('any', 1)\n('data', 1)\n('source', 1)\n('offering', 1)\n('a', 1)\n('Hadoop', 1)\n('InputFormat.', 1)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["('Spark', 1)\n('revolves', 1)\n('around', 1)\n('the', 1)\n('concept', 1)\n('of', 1)\n('a', 1)\n('resilient', 1)\n('distributed', 1)\n('dataset', 1)\n('(RDD),', 1)\n('', 1)\n('which', 1)\n('is', 1)\n('a', 1)\n('fault-tolerant', 1)\n('collection', 1)\n('of', 1)\n('elements', 1)\n('that', 1)\n('can', 1)\n('be', 1)\n('operated', 1)\n('on', 1)\n('in', 1)\n('parallel.', 1)\n('', 1)\n('There', 1)\n('are', 1)\n('two', 1)\n('ways', 1)\n('to', 1)\n('create', 1)\n('RDDs:', 1)\n('parallelizing', 1)\n('an', 1)\n('existing', 1)\n('collection', 1)\n('in', 1)\n('your', 1)\n('driver', 1)\n('program,', 1)\n('', 1)\n('or', 1)\n('referencing', 1)\n('a', 1)\n('dataset', 1)\n('in', 1)\n('an', 1)\n('external', 1)\n('storage', 1)\n('system,', 1)\n('such', 1)\n('as', 1)\n('a', 1)\n('shared', 1)\n('filesystem,', 1)\n('HDFS,', 1)\n('HBase,', 1)\n('', 1)\n('or', 1)\n('any', 1)\n('data', 1)\n('source', 1)\n('offering', 1)\n('a', 1)\n('Hadoop', 1)\n('InputFormat.', 1)\n"]}}],"execution_count":0},{"cell_type":"code","source":["#reduceByKey\nrdd4=rdd3.reduceByKey(lambda a,b: a+b)\nfor element in rdd4.collect():\n    print(element)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae0e2002-ba24-41c2-abe5-cb192751a361"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"('Spark', 1)\n('around', 1)\n('of', 2)\n('', 4)\n('is', 1)\n('fault-tolerant', 1)\n('collection', 2)\n('operated', 1)\n('in', 3)\n('are', 1)\n('two', 1)\n('an', 2)\n('driver', 1)\n('program,', 1)\n('external', 1)\n('storage', 1)\n('system,', 1)\n('as', 1)\n('filesystem,', 1)\n('HBase,', 1)\n('source', 1)\n('offering', 1)\n('InputFormat.', 1)\n('revolves', 1)\n('the', 1)\n('concept', 1)\n('a', 5)\n('resilient', 1)\n('distributed', 1)\n('dataset', 2)\n('(RDD),', 1)\n('which', 1)\n('elements', 1)\n('that', 1)\n('can', 1)\n('be', 1)\n('on', 1)\n('parallel.', 1)\n('There', 1)\n('ways', 1)\n('to', 1)\n('create', 1)\n('RDDs:', 1)\n('parallelizing', 1)\n('existing', 1)\n('your', 1)\n('or', 2)\n('referencing', 1)\n('such', 1)\n('shared', 1)\n('HDFS,', 1)\n('any', 1)\n('data', 1)\n('Hadoop', 1)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["('Spark', 1)\n('around', 1)\n('of', 2)\n('', 4)\n('is', 1)\n('fault-tolerant', 1)\n('collection', 2)\n('operated', 1)\n('in', 3)\n('are', 1)\n('two', 1)\n('an', 2)\n('driver', 1)\n('program,', 1)\n('external', 1)\n('storage', 1)\n('system,', 1)\n('as', 1)\n('filesystem,', 1)\n('HBase,', 1)\n('source', 1)\n('offering', 1)\n('InputFormat.', 1)\n('revolves', 1)\n('the', 1)\n('concept', 1)\n('a', 5)\n('resilient', 1)\n('distributed', 1)\n('dataset', 2)\n('(RDD),', 1)\n('which', 1)\n('elements', 1)\n('that', 1)\n('can', 1)\n('be', 1)\n('on', 1)\n('parallel.', 1)\n('There', 1)\n('ways', 1)\n('to', 1)\n('create', 1)\n('RDDs:', 1)\n('parallelizing', 1)\n('existing', 1)\n('your', 1)\n('or', 2)\n('referencing', 1)\n('such', 1)\n('shared', 1)\n('HDFS,', 1)\n('any', 1)\n('data', 1)\n('Hadoop', 1)\n"]}}],"execution_count":0},{"cell_type":"code","source":["#sortByKey\nrdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()\nfor element in rdd5.collect():\n    print(element)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fc909c8-e21d-4a08-a413-650281d75ab9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"(1, 'Spark')\n(1, 'around')\n(1, 'is')\n(1, 'fault-tolerant')\n(1, 'operated')\n(1, 'are')\n(1, 'two')\n(1, 'driver')\n(1, 'program,')\n(1, 'external')\n(1, 'storage')\n(1, 'system,')\n(1, 'as')\n(1, 'filesystem,')\n(1, 'HBase,')\n(1, 'source')\n(1, 'offering')\n(1, 'InputFormat.')\n(1, 'revolves')\n(1, 'the')\n(1, 'concept')\n(1, 'resilient')\n(1, 'distributed')\n(1, '(RDD),')\n(1, 'which')\n(1, 'elements')\n(1, 'that')\n(1, 'can')\n(1, 'be')\n(1, 'on')\n(1, 'parallel.')\n(1, 'There')\n(1, 'ways')\n(1, 'to')\n(1, 'create')\n(1, 'RDDs:')\n(1, 'parallelizing')\n(1, 'existing')\n(1, 'your')\n(1, 'referencing')\n(1, 'such')\n(1, 'shared')\n(1, 'HDFS,')\n(1, 'any')\n(1, 'data')\n(1, 'Hadoop')\n(2, 'of')\n(2, 'collection')\n(2, 'an')\n(2, 'dataset')\n(2, 'or')\n(3, 'in')\n(4, '')\n(5, 'a')\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["(1, 'Spark')\n(1, 'around')\n(1, 'is')\n(1, 'fault-tolerant')\n(1, 'operated')\n(1, 'are')\n(1, 'two')\n(1, 'driver')\n(1, 'program,')\n(1, 'external')\n(1, 'storage')\n(1, 'system,')\n(1, 'as')\n(1, 'filesystem,')\n(1, 'HBase,')\n(1, 'source')\n(1, 'offering')\n(1, 'InputFormat.')\n(1, 'revolves')\n(1, 'the')\n(1, 'concept')\n(1, 'resilient')\n(1, 'distributed')\n(1, '(RDD),')\n(1, 'which')\n(1, 'elements')\n(1, 'that')\n(1, 'can')\n(1, 'be')\n(1, 'on')\n(1, 'parallel.')\n(1, 'There')\n(1, 'ways')\n(1, 'to')\n(1, 'create')\n(1, 'RDDs:')\n(1, 'parallelizing')\n(1, 'existing')\n(1, 'your')\n(1, 'referencing')\n(1, 'such')\n(1, 'shared')\n(1, 'HDFS,')\n(1, 'any')\n(1, 'data')\n(1, 'Hadoop')\n(2, 'of')\n(2, 'collection')\n(2, 'an')\n(2, 'dataset')\n(2, 'or')\n(3, 'in')\n(4, '')\n(5, 'a')\n"]}}],"execution_count":0},{"cell_type":"code","source":["#filter\nrdd6 = rdd5.filter(lambda x : 's' == x[1])  #in\nfor element in rdd6.collect():\n    print(element)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42838761-3475-450d-94b7-fc71a2256f48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"(5, 'a')\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["(5, 'a')\n"]}}],"execution_count":0},{"cell_type":"code","source":["stwords = ['a','an','the']\nrdd3 = rdd2.filter(lambda x: x not in stwords)\n\nfor element in rdd3.collect():\n    print(element)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d26b3b6a-5541-4333-a15c-57a24e2dd1ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Spark\nrevolves\naround\nconcept\nresilient\ndistributed\ndataset\n(RDD),\n\nwhich\nis\nfault-tolerant\ncollection\nelements\nthat\ncan\nbe\noperated\non\nin\nparallel.\n\nThere\nare\ntwo\nways\nto\ncreate\nRDDs:\nparallelizing\nexisting\ncollection\nin\nyour\ndriver\nprogram,\n\nor\nreferencing\ndataset\nin\nexternal\nstorage\nsystem,\nsuch\nas\nshared\nfilesystem,\nHDFS,\nHBase,\n\nor\nany\ndata\nsource\noffering\nHadoop\nInputFormat.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Spark\nrevolves\naround\nconcept\nresilient\ndistributed\ndataset\n(RDD),\n\nwhich\nis\nfault-tolerant\ncollection\nelements\nthat\ncan\nbe\noperated\non\nin\nparallel.\n\nThere\nare\ntwo\nways\nto\ncreate\nRDDs:\nparallelizing\nexisting\ncollection\nin\nyour\ndriver\nprogram,\n\nor\nreferencing\ndataset\nin\nexternal\nstorage\nsystem,\nsuch\nas\nshared\nfilesystem,\nHDFS,\nHBase,\n\nor\nany\ndata\nsource\noffering\nHadoop\nInputFormat.\n"]}}],"execution_count":0},{"cell_type":"code","source":["rdd4 = rdd3.filter(lambda x: x in stwords)\nfor element in rdd4.collect():\n    print(element)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e43cf03-bd98-46bd-b03e-e8748a4c4132"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ex = spark.sparkContext.textFile('dbfs:/FileStore/shared_uploads/AKSHATAKHEDEKAR01032001@rjcollege.edu.in/example_2-1.txt')\nex.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8a94480-1cfd-4063-bcd1-80895001adbc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[37]: ['IT_Sumit',\n 'IT_Amit',\n 'DSai_Suraj',\n 'DSAI_Saurabh',\n 'CS_Chaitnya',\n 'CS_Namita']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[37]: ['IT_Sumit',\n 'IT_Amit',\n 'DSai_Suraj',\n 'DSAI_Saurabh',\n 'CS_Chaitnya',\n 'CS_Namita']"]}}],"execution_count":0},{"cell_type":"code","source":["#Get the list of students of DSAI\nfrom pyspark.sql.functions import upper\nex_1 = ex.filter(lambda x: 'DSAI' in x.upper())\nex_1.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfa30e25-d189-4ea7-935b-7ee33e50a50b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[59]: ['DSai_Suraj', 'DSAI_Saurabh']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[59]: ['DSai_Suraj', 'DSAI_Saurabh']"]}}],"execution_count":0},{"cell_type":"code","source":["#Count the number of students of IT\nex_2 = ex.filter(lambda x: 'IT' in x)\nex_2.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd55098b-bb07-4198-aa9b-545ae75101c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[57]: 2","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[57]: 2"]}}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.parallelize(['IT_Sumit',\n 'IT_Amit',\n 'DSai_Suraj',\n 'DSAI_Saurabh',\n 'CS_Chaitnya',\n 'CS_Namita'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"557ed497-6b51-4485-83dd-69383808d09e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Get the 10% sample of rdd data\nrdd_sample1 = rdd.sample(False, .3, 2)\n\n#Get the 20% sample of rdd data\nrdd_sample2 = rdd.sample(False, .5, 3)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"663eaa80-ae19-4b1b-bf37-cb8deed6e096"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\n#Combine the data of 10% sample rdd and 20% sample rdd into the rdd named 'union rdd'\nunion_samples = rdd_sample1.union(rdd_sample2)\nunion_samples.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83abbc53-711f-4381-9157-d3e3bd80e042"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[65]: ['DSAI_Saurabh', 'IT_Amit', 'DSAI_Saurabh']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[65]: ['DSAI_Saurabh', 'IT_Amit', 'DSAI_Saurabh']"]}}],"execution_count":0},{"cell_type":"code","source":["#Get only distinct values from the union RDD into the rdd named 'distinctRDD'\ndistinctRDD = rdd.distinct()\ndistinctRDD.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc6b2759-f09e-4554-8ada-2e8cf0786eb6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[55]: ['CS_Chaitnya',\n 'DSai_Suraj',\n 'CS_Namita',\n 'IT_Sumit',\n 'IT_Amit',\n 'DSAI_Saurabh']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[55]: ['CS_Chaitnya',\n 'DSai_Suraj',\n 'CS_Namita',\n 'IT_Sumit',\n 'IT_Amit',\n 'DSAI_Saurabh']"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\nnames = Row(\"Year\",\"First_Name\",\"Country\",\"Sex\",\"Count\")\nname1 = names(2012,\"DOMINIC\",\"CAGYUGA\",\"M\",6)\nname2 = names(2012,\"ADDISON\",\"ONODAGA\",\"F\",14)\nname3 = names(2012,\"ADDISON\",\"ONONDAGA\",\"F\",14)\nname4 = names(2013,\"JULIA\",\"ONONDAGA\",\"F\",15)\nname5 = names(2014,\"MULIA\",\"DAGA\",\"M\",10)\n\nnamesall = Row(names=[name1,name2,name3,name4,name5])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8e5a2b4-a4cd-47bd-81b6-1b22afde38ae"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Q. Get the list of first names from the above RDD using map function.\n# for i in range(0, len(namesall.names)):\n#     print(namesall.names[i].First_Name)\n    \nrdd354 = map(lambda x : x.names.First_Name,rdd);rdd354.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb9521ec-049e-4da6-8388-7b0972841567"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3227342258921575>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m#     print(namesall.names[i].First_Name)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mrdd354\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFirst_Name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m;\u001B[0m\u001B[0mrdd354\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mTypeError\u001B[0m: 'RDD' object is not iterable","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: 'RDD' object is not iterable","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3227342258921575>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m#     print(namesall.names[i].First_Name)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mrdd354\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFirst_Name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m;\u001B[0m\u001B[0mrdd354\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mTypeError\u001B[0m: 'RDD' object is not iterable"]}}],"execution_count":0},{"cell_type":"code","source":["type(namesall)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95f8c4ef-0b15-491b-8633-965679f1e6e7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[89]: pyspark.sql.types.Row","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[89]: pyspark.sql.types.Row"]}}],"execution_count":0},{"cell_type":"code","source":["rdd= sc.parallelize(namesall)\nrdd[0].collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07fe03c8-ff1a-4335-b498-6f1a8d7738f0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-268490988253062>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mrdd\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnamesall\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mrdd\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mTypeError\u001B[0m: 'RDD' object is not subscriptable","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: 'RDD' object is not subscriptable","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-268490988253062>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mrdd\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnamesall\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mrdd\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mTypeError\u001B[0m: 'RDD' object is not subscriptable"]}}],"execution_count":0},{"cell_type":"code","source":["#Q. Count the total number of female candidates.\nrdd1 = rdd.map(lambda a:(a[1][1]))\nrdd1.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9543a066-ec55-4bb1-976d-8f0662fdfab8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[114]: ['ADDISON']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[114]: ['ADDISON']"]}}],"execution_count":0},{"cell_type":"code","source":["#Q. Get the of distinct counties.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eebabb54-b5c7-4755-94d0-db78337e28fb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Q. Get the distinct list of year data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db691f13-471a-4a3a-951d-6b728c6caebb"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"RDD Transformation (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2290417408869598}},"nbformat":4,"nbformat_minor":0}
